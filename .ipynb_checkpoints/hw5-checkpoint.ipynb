{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446529, 3)\n",
      "(446529, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/hw5/HMP_Dataset'\n",
    "report_dir = 'reports/hw5'\n",
    "\n",
    "pathlist = Path(data_dir).glob('*/')\n",
    "acc = np.empty((0,3))\n",
    "labels = np.empty((0,1))\n",
    "for index, path in enumerate(pathlist):\n",
    "    if(path.name.startswith('.') or not path.is_dir()):\n",
    "        continue\n",
    "    files = path.glob('*.txt')\n",
    "    for j, file in enumerate(files):\n",
    "        part = np.loadtxt(file)\n",
    "        acc = np.vstack((acc, part))\n",
    "        labels = np.vstack((labels, np.full((part.shape[0],1), path.name)))\n",
    "print(acc.shape)\n",
    "print(labels.shape)\n",
    "assert labels.shape[0] == acc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Brush_teeth', 'Climb_stairs', 'Comb_hair', 'Descend_stairs',\n",
       "       'Drink_glass', 'Eat_meat', 'Eat_soup', 'Getup_bed', 'Liedown_bed',\n",
       "       'Pour_water', 'Sitdown_chair', 'Standup_chair', 'Use_telephone',\n",
       "       'Walk'], dtype='<U32')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = np.hstack((acc, labels))\n",
    "# data[0]\n",
    "\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_samples(data, sample_length):\n",
    "    n_slices = math.floor((data.shape[0])/sample_length)\n",
    "    total_size = n_slices * sample_length\n",
    "    slices = np.vsplit(np.vsplit(data, [total_size])[0], n_slices)\n",
    "    reshaped_slices = map(lambda x: x.reshape(1, sample_length*3).flatten(), np.array(slices))\n",
    "    transformed = np.array(list(reshaped_slices))\n",
    "    assert transformed.shape == (len(slices), len(slices[0])*len(slices[0][1]))\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histograms_by_group(group):\n",
    "    histograms = np.array(group.histogram)\n",
    "    new_histograms = np.zeros((histograms.shape[0],histograms[0].shape[0]))\n",
    "    \n",
    "    for i, histogram in enumerate(histograms):\n",
    "        for j, count in enumerate(histogram):\n",
    "            new_histograms[i][j] = count\n",
    "    \n",
    "    return new_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(group):\n",
    "    return get_histograms_by_group(group).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(samples, n_K, k_means):\n",
    "    labels = k_means.predict(samples)\n",
    "    histogram = np.histogram(labels, bins=range(n_K+1))\n",
    "    return histogram\n",
    "\n",
    "def get_histograms_and_labels(k_means, sample_length, n_clusters):\n",
    "    pathlist = Path(data_dir).glob('*/')\n",
    "    histograms = []\n",
    "    labels = []\n",
    "\n",
    "    for path in pathlist:\n",
    "        if(path.name.startswith('.') or not path.is_dir()):\n",
    "            continue\n",
    "        label = path.name\n",
    "        files = path.glob('*.txt')\n",
    "        \n",
    "        for file in files:\n",
    "            raw = np.loadtxt(file)\n",
    "            transformed = transform_to_samples(np.array(raw), sample_length=sample_length)\n",
    "            histogram = get_histogram(transformed, n_clusters, k_means)\n",
    "            histograms.append(histogram)\n",
    "            labels.append(label)\n",
    "    return np.array(histograms), np.array(labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_vector(sample_length, overlap, n_clusters):\n",
    "    transformed = transform_to_samples(acc, sample_length=sample_length)\n",
    "    k_means = cluster.KMeans(n_clusters=n_clusters)\n",
    "    k_means.fit(transformed)\n",
    "    historams, labels = get_histograms_and_labels(k_means, sample_length, n_clusters)\n",
    "    historams.shape, labels.shape\n",
    "    assert historams[:, 0][0].shape == (n_clusters,)\n",
    "    df = pd.DataFrame(np.hstack((historams[:, 0].reshape(839,1), labels.reshape(839, 1))), columns=['histogram', 'label'])\n",
    "    h_groups = df.groupby(['label'])\n",
    "    mean_histograms = h_groups.apply(get_mean)\n",
    "    histograms_by_group = h_groups.apply(get_histograms_by_group)\n",
    "    return mean_histograms, histograms_by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_histogram(mean_histograms_df, n_K):\n",
    "    indexes = mean_histograms_df.index\n",
    "    for i, histogram in enumerate(mean_histograms_df):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'mean_histograms {indexes[i]}')\n",
    "        plt.bar(range(n_K), histogram)\n",
    "        plt.savefig(f'{report_dir}/{indexes[i]}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all_histograms(histograms_by_group_df, kf, n_K):\n",
    "    sets = dict.fromkeys(range(3))\n",
    "    for key in list(sets.keys()):\n",
    "        sets[key] = [np.empty((0, n_K)),np.empty((0, n_K)),np.empty((0, 1)), np.empty((0, 1))]\n",
    "\n",
    "    for label, histograms in histograms_by_group_df.items():\n",
    "        X = histograms\n",
    "        Y = np.full((histograms.shape[0],1), label)\n",
    "        split = kf.split(X)\n",
    "        for index, (train_index, test_index) in enumerate(split):\n",
    "            X_each_train, X_each_test = X[train_index], X[test_index]\n",
    "            Y_each_train, Y_each_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            sets[index][0] = np.vstack((sets[index][0], X_each_train))\n",
    "            sets[index][1] = np.vstack((sets[index][1], X_each_test))\n",
    "            sets[index][2] = np.vstack((sets[index][2], Y_each_train))\n",
    "            sets[index][3] = np.vstack((sets[index][3], Y_each_test))\n",
    "        \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confustion_matrix(n_estimators,max_depth, sets):\n",
    "     for i in range(3):\n",
    "        [X_train, X_test, Y_train, Y_test] = sets[i]\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=10)\n",
    "        clf.fit(X_train, Y_train.squeeze())\n",
    "        Y_predicts = clf.predict(X_test)\n",
    "        print(np.around(normalize(confusion_matrix(Y_test, Y_predicts,labels=['Brush_teeth', 'Climb_stairs', 'Comb_hair', 'Descend_stairs',\n",
    "       'Drink_glass', 'Eat_meat', 'Eat_soup', 'Getup_bed', 'Liedown_bed',\n",
    "       'Pour_water', 'Sitdown_chair', 'Standup_chair', 'Use_telephone',\n",
    "       'Walk'])), decimals=2))\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sets, get_confustion_matrix, n_estimators=200, max_depth=100):\n",
    "    if(get_confustion_matrix):\n",
    "        print_confustion_matrix(n_estimators, max_depth,sets)\n",
    "        return\n",
    "    scores = []\n",
    "    for i in range(3):\n",
    "        [X_train, X_test, Y_train, Y_test] = sets[i]\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=10)\n",
    "        clf.fit(X_train, Y_train.squeeze())\n",
    "        score = clf.score(X_test, Y_test.squeeze())\n",
    "        scores.append(score)\n",
    "        print(f'itr: {i}, score: {score}')\n",
    "    mean_accuracy = np.array(scores).mean()\n",
    "    print(f'average score: {mean_accuracy}')\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(sample_length, overlap, n_clusters, plot_fig=False, get_confustion_matrix=False):\n",
    "    # Vector quantization\n",
    "    mean_histograms, histograms_by_group = get_quantized_vector(sample_length, overlap, n_clusters)\n",
    "    # Plot mean histogram\n",
    "    if(plot_fig):\n",
    "        plot_mean_histogram(mean_histograms, n_clusters)\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    # Split data\n",
    "    sets = split_all_histograms(histograms_by_group, kf, n_clusters)\n",
    "    # assertion\n",
    "    [X_train, X_test, Y_train, Y_test] = sets[0]\n",
    "    assert X_train.shape[0] + X_test.shape[0] == 839\n",
    "    # Evaluate\n",
    "    accracy = evaluate(sets, get_confustion_matrix)\n",
    "    print(f'sample_length: {sample_length}, overlap: {overlap}, n_clusters: {n_clusters} =====> accracy: {accracy}')\n",
    "    return accracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 0, score: 0.7263157894736842\n",
      "itr: 1, score: 0.7266187050359713\n",
      "itr: 2, score: 0.7681159420289855\n",
      "average score: 0.7403501455128803\n",
      "sample_length: 32, overlap: 0, n_clusters: 480 =====> accracy: 0.7403501455128803\n",
      "CPU times: user 1min 18s, sys: 8.82 s, total: 1min 27s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accracy = experiment(sample_length=32, overlap=0, n_clusters=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 0, score: 0.7157894736842105\n",
      "itr: 1, score: 0.7805755395683454\n",
      "itr: 2, score: 0.7282608695652174\n",
      "average score: 0.7415419609392577\n",
      "sample_length: 32, overlap: 0, n_clusters: 240 =====> accracy: 0.7415419609392577\n",
      "CPU times: user 50.2 s, sys: 6.08 s, total: 56.3 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accracy = experiment(sample_length=32, overlap=0, n_clusters=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 0, score: 0.7578947368421053\n",
      "itr: 1, score: 0.7230215827338129\n",
      "itr: 2, score: 0.8043478260869565\n",
      "average score: 0.7617547152209583\n",
      "sample_length: 16, overlap: 0, n_clusters: 480 =====> accracy: 0.7617547152209583\n",
      "CPU times: user 2min 36s, sys: 20.9 s, total: 2min 57s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accracy = experiment(sample_length=16, overlap=0, n_clusters=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95 0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.06 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.37 0.   0.93 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.1  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.96 0.   0.   0.08 0.27 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.57 0.   0.19 0.57 0.57 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.98 0.18 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.18 0.   0.04 0.35 0.92 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.83 0.   0.   0.   0.   0.55 0.   0.   0.   0.  ]\n",
      " [0.   0.18 0.   0.04 0.   0.   0.   0.   0.   0.   0.   0.04 0.   0.98]]\n",
      "[[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.03 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.92 0.   0.39 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.4  0.   0.91 0.   0.   0.   0.   0.   0.   0.1  0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.03 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.71 0.   0.   0.   0.71 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.96 0.   0.   0.   0.29 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.51 0.   0.17 0.85 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.13 0.   0.   0.89 0.44 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.29 0.96 0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.14 0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.   0.   0.99]]\n",
      "[[0.95 0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.03 0.   0.   0.   0.   0.   0.03 0.   0.   0.   0.03]\n",
      " [0.   0.   0.95 0.   0.27 0.   0.   0.   0.   0.   0.14 0.   0.   0.  ]\n",
      " [0.   0.53 0.   0.84 0.   0.   0.   0.   0.   0.   0.   0.11 0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.17 0.   0.   0.96 0.   0.04 0.04 0.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.16 0.   0.   0.94 0.   0.   0.31 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.97 0.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.04 0.   0.   0.   0.   0.   0.4  0.92 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.82 0.41 0.   0.   0.  ]\n",
      " [0.   0.21 0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.17 0.   0.96]]\n",
      "sample_length: 16, overlap: 0, n_clusters: 240 =====> accracy: None\n",
      "CPU times: user 1min 38s, sys: 14.3 s, total: 1min 53s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accracy = experiment(sample_length=16, overlap=0, n_clusters=240, get_confustion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
